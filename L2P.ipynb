{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2P impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets\n",
    "    - Split CIFAR-100 : Split to 10 tasks (10 classes per task)\n",
    "    - 5-datasets : CIFAR-10, MNIST, Fashion-MNIST, SVHN, notMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# dataset name, transform_train, transform_val, data_path\n",
    "def get_dataset(dataset, transform_train, transform_val, data_path, download):\n",
    "    if dataset == \"CIFAR10\":\n",
    "        train = datasets.CIFAR10(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.CIFAR10(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        train = datasets.CIFAR100(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.CIFAR100(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"MNIST\":\n",
    "        train = datasets.MNIST(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.MNIST(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"Fashion-MNIST\":\n",
    "        train = datasets.FashionMNIST(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.FashionMNIST(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"SVHN\":\n",
    "        train = datasets.SVHN(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.SVHN(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"notMNIST\":\n",
    "        root = data_path\n",
    "        if download:\n",
    "            # data url\n",
    "            data_url = \"https://github.com/facebookresearch/Adversarial-Continual-Learning/raw/main/data/notMNIST.zip\"\n",
    "            zip_file_path = \"{}/notMNIST.zip\".format(root)\n",
    "            # retrieve data \n",
    "            print(\"Downloading notMNIST from https://github.com/facebookresearch/Adversarial-Continual-Learning/raw/main/data/notMNIST.zip\")\n",
    "            path, headers = urlretrieve(data_url, zip_file_path)\n",
    "            # unzip\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as obj:\n",
    "                obj.extractall(root)\n",
    "        \n",
    "        train = datasets.ImageFolder(\"{}/notMNIST/Train\".format(root), transform=transform_train)\n",
    "        val = datasets.ImageFolder(\"{}/notMNIST/Train\".format(root), transform=transform_val)\n",
    "        \n",
    "    else :\n",
    "        raise ValueError(\"{} not found\".format(dataset))\n",
    "    return train, val\n",
    "\n",
    "def get_tranforms(is_train):\n",
    "    # train dataset transform\n",
    "    if is_train:\n",
    "        return transforms.Compose([\n",
    "                transforms.PILToTensor(),\n",
    "                transforms.RandomResizedCrop(size=(224,224)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "            ])\n",
    "    # test dataset transform\n",
    "    else :\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "def print_img(dataset, idx):\n",
    "    img = dataset.__getitem__(idx)[0]\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(to_pil_image(img), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model : Vision Transformer ViT-B/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n",
      "torch.Size([8, 196, 768])\n",
      "torch.Size([8, 197, 768])\n",
      "torch.Size([8, 197, 2304])\n",
      "torch.Size([3, 8, 8, 197, 96])\n",
      "torch.Size([8, 8, 197, 197])\n",
      "torch.Size([8, 8, 197, 197])\n",
      "torch.Size([8, 8, 197, 96])\n",
      "torch.Size([8, 197, 768])\n",
      "torch.Size([8, 197, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\99san\\AppData\\Local\\Temp\\ipykernel_8204\\453281835.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = nn.Softmax()(e)\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "x = torch.randn(8,3,224,224)\n",
    "print(x.shape)\n",
    "\n",
    "embed_layer = PatchEmbed(224,16)\n",
    "x = embed_layer(x)\n",
    "print(x.shape)\n",
    "\n",
    "cls_token = nn.Parameter(torch.randn(8,1,768))\n",
    "x = torch.cat((cls_token, x), dim=1)\n",
    "pos_embedding = nn.Parameter(torch.randn(1,196 + 1,768))\n",
    "x += pos_embedding # (batch, num_patches+1, embed_dim)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "\n",
    "# takes an image as an input, divide it into patches, let it through the embedding layer\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            stride=patch_size, \n",
    "            kernel_size=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.proj(input)               # (n, 3, 224, 224) -> (n, embed_dim, patch_size, patch_size, ) \n",
    "        x = torch.flatten(x, start_dim=2, end_dim=3)  # (n, embed_dim, patch_size*patch_size)\n",
    "        x = torch.transpose(x,1,2)           # (n, patch_size*patch_size, embed_dim)\n",
    "        return x\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim : int, \n",
    "                 qkv_bias : bool = True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.dropout_attn = nn.Dropout(0.)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout_proj = nn.Dropout(0.)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input : (n, num_patch+1, embed_dim)\n",
    "        \n",
    "        num_samples, num_tokens, dim = input.shape\n",
    "        if dim != self.dim :\n",
    "            raise ValueError\n",
    "        \n",
    "        # multi-head attention\n",
    "        # create qkv\n",
    "        x = self.qkv(input)\n",
    "        # decouple q k v \n",
    "        x = einops.rearrange(x, 'b  n (h d qkv) -> (qkv) b h n d', h=8, qkv=3)\n",
    "        q,k,v = x[0], x[1], x[2]\n",
    "        \n",
    "        # Scaled dot product attention\n",
    "        # q * k / d ** 1/2\n",
    "        e = torch.einsum('bnqd, bnkd -> bnqk', q, k) / ((self.dim)**(1/2))\n",
    "        x = nn.Softmax()(e)\n",
    "        # * v\n",
    "        x = torch.einsum('bnqk, bnvd -> bnqd',x,v)\n",
    "        x = einops.rearrange(x,'b n q d -> b q (n d)')\n",
    "        # linear \n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim : int,\n",
    "                 expansion : int = 4, \n",
    "                 drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = AttentionLayer(embed_dim)\n",
    "        ## mlp\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(expansion * embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input): #\n",
    "        x = self.norm1(input)\n",
    "        x = self.attention(x)\n",
    "        x += input\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x += residual\n",
    "        return x\n",
    "        \n",
    "class Head(nn.Module):\n",
    "    def __init__(self,\n",
    "                embed_dim : int, \n",
    "                num_class : int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_class = num_class\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = einops.reduce(input ,'b n e -> b e', reduction=\"mean\")\n",
    "        x = nn.LayerNorm(self.embed_dim, x)\n",
    "        x = nn.Linear(self.embed_dim, self.num_class)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth:int, embed_dim:int):\n",
    "        super().__init__(*[TransformerEncoderBlock(embed_dim) for _ in range(depth)])\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                num_class : int,\n",
    "                batch_size : int,\n",
    "                img_size : int = 224, \n",
    "                patch_size : int = 16,\n",
    "                in_channels : int = 3,\n",
    "                embed_dim : int = 768,\n",
    "                depth : int = 12,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.embedding = PatchEmbed(img_size, patch_size, in_channels=in_channels, embed_dim=embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(batch_size,1,embed_dim))\n",
    "        self.num_patch = (img_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,self.num_patch + 1,embed_dim))\n",
    "        self.transformer_encoder = TransformerEncoder(depth, embed_dim)\n",
    "        self.head = Head(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, input): # input (img): (batch, 3, H, W)\n",
    "        x = self.embedding.forward(input) # (batch, num_patches, embed_dim)\n",
    "        # add cls (batch, num_patches, embed_dim) + (batch, 1, embed_dim) = (batch,num_patches+1, embed_dim)\n",
    "        x = torch.cat((self.cls_token, x), dim=1)\n",
    "        # pos (batch, num_patches+1, embed_dim) + (num_patches+1, embed_dim)\n",
    "        x += self.pos_embedding # (batch, num_patches+1, embed_dim)\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.head.forward(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_class = 1000\n",
    "        self.batch_size = 128\n",
    "        self.img_size = 224\n",
    "        self.patch_size = 16\n",
    "        self.in_channels = 3\n",
    "        self.embed_dim = 768\n",
    "        self.depth = 12\n",
    "    \n",
    "configs = Config()\n",
    "model = VisionTransformer(configs.num_class, configs.batch_size)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
