{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2P impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets\n",
    "    - Split CIFAR-100 : Split to 10 tasks (10 classes per task)\n",
    "    - 5-datasets : CIFAR-10, MNIST, Fashion-MNIST, SVHN, notMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# dataset name, transform_train, transform_val, data_path\n",
    "def get_dataset(dataset, transform_train, transform_val, data_path, download):\n",
    "    if dataset == \"CIFAR10\":\n",
    "        train = datasets.CIFAR10(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.CIFAR10(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        train = datasets.CIFAR100(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.CIFAR100(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"MNIST\":\n",
    "        train = datasets.MNIST(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.MNIST(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"Fashion-MNIST\":\n",
    "        train = datasets.FashionMNIST(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.FashionMNIST(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"SVHN\":\n",
    "        train = datasets.SVHN(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.SVHN(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"notMNIST\":\n",
    "        root = data_path\n",
    "        if download:\n",
    "            # data url\n",
    "            data_url = \"https://github.com/facebookresearch/Adversarial-Continual-Learning/raw/main/data/notMNIST.zip\"\n",
    "            zip_file_path = \"{}/notMNIST.zip\".format(root)\n",
    "            # retrieve data \n",
    "            print(\"Downloading notMNIST from https://github.com/facebookresearch/Adversarial-Continual-Learning/raw/main/data/notMNIST.zip\")\n",
    "            path, headers = urlretrieve(data_url, zip_file_path)\n",
    "            # unzip\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as obj:\n",
    "                obj.extractall(root)\n",
    "        \n",
    "        train = datasets.ImageFolder(\"{}/notMNIST/Train\".format(root), transform=transform_train)\n",
    "        val = datasets.ImageFolder(\"{}/notMNIST/Train\".format(root), transform=transform_val)\n",
    "        \n",
    "    else :\n",
    "        raise ValueError(\"{} not found\".format(dataset))\n",
    "    return train, val\n",
    "\n",
    "def get_tranforms(is_train):\n",
    "    # train dataset transform\n",
    "    if is_train:\n",
    "        return transforms.Compose([\n",
    "                transforms.PILToTensor(),\n",
    "                transforms.RandomResizedCrop(size=(224,224)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "            ])\n",
    "    # test dataset transform\n",
    "    else :\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "def print_img(dataset, idx):\n",
    "    img = dataset.__getitem__(idx)[0]\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(to_pil_image(img), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model : Vision Transformer ViT-B/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (749670914.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[29], line 77\u001b[1;36m\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# takes an image as an input, divide it into patches, let it through the embedding layer\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            stride=patch_size, \n",
    "            kernel_size=patch_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.proj(input)               # (n, 3, 224, 224) -> (n, embed_dim, patch_size, patch_size, ) \n",
    "        x = torch.flatten(x, start_dim=2, end_dim=3)  # (n, embed_dim, patch_size*patch_size)\n",
    "        x = torch.transpose(x,1,2)           # (n, patch_size*patch_size, embed_dim)\n",
    "        return x\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, dim, qkv_bias=True):\n",
    "        super.__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.dropout_attn = nn.Dropout(0.)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout_proj = nn.Dropout(0.)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input : (n, num_patch+1, embed_dim)\n",
    "        ''' \n",
    "        [\n",
    "            [  1번 이미지       \n",
    "                [     1번 패치\n",
    "                        embed_dim\n",
    "                ]\n",
    "            ],\n",
    "            [   2번 이미지\n",
    "                [   1번패치\n",
    "                    1,2,3214,,32,63,3261,5,..\n",
    "                ]\n",
    "                \n",
    "            ],\n",
    "            ...\n",
    "        ]\n",
    "        '''\n",
    "        num_samples, num_tokens, dim = input.shape\n",
    "        if dim != self.dim :\n",
    "            raise ValueError\n",
    "        \n",
    "        # Norm\n",
    "        # multi-head attention\n",
    "        q,k,v = self.qkv(input), self.qkv(input), self.qkv(input)  # query, key, value\n",
    "        \n",
    "        # Residual connection\n",
    "        # Norm\n",
    "        # MLP\n",
    "        # Residual connection\n",
    "        \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim, batch_size):\n",
    "        self.embedding = PatchEmbed(img_size, patch_size, in_channels=in_channels, embed_dim=embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(batch_size,1,embed_dim))\n",
    "        self.num_patch = (img_size / patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,self.num_patch + 1,embed_dim))\n",
    "        self.attention\n",
    "\n",
    "    def forward(self, input): # input (img): (batch, 3, H, W)\n",
    "        x = self.embedding.forward(input) # (batch, num_patches, embed_dim)\n",
    "        # add cls (batch, num_patches, embed_dim) + (batch, 1, embed_dim) = (batch,num_patches+1, embed_dim)\n",
    "        x = torch.cat((self.cls_token, x), dim=1)\n",
    "        # pos (batch, num_patches+1, embed_dim) + (num_patches+1, embed_dim)\n",
    "        x += self.pos_embedding\n",
    "        # attention 적용\n",
    "\n",
    "        x = self.attentio\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
