{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1qK2pejmPCt"
      },
      "source": [
        "# L2P impl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oX_-34vmPCw"
      },
      "source": [
        "# 1. Datasets\n",
        "    - Split CIFAR-100 : Split to 10 tasks (10 classes per task)\n",
        "    - 5-datasets : CIFAR-10, MNIST, Fashion-MNIST, SVHN, notMNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlzE948amPCw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DGEawHmZrK",
        "outputId": "2b24acce-1215-4de5-c772-f8c942936987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgDSE2ARmcmv",
        "outputId": "22bc5c85-7c67-4383-f319-3e8fa02970e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNLPOD6amPCx"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "from urllib.request import urlretrieve\n",
        "import zipfile\n",
        "\n",
        "# dataset name, transform_train, transform_val, data_path\n",
        "def get_dataset(dataset, transform_train, transform_val, data_path, download):\n",
        "    if dataset == \"CIFAR10\":\n",
        "        train = datasets.CIFAR10(data_path, train=True, download=download, transform=transform_train)\n",
        "        val = datasets.CIFAR10(data_path, train=False, download=download, transform=transform_train)\n",
        "    elif dataset == \"CIFAR100\":\n",
        "        train = datasets.CIFAR100(data_path, train=True, download=download, transform=transform_train)\n",
        "        val = datasets.CIFAR100(data_path, train=False, download=download, transform=transform_train)\n",
        "    elif dataset == \"MNIST\":\n",
        "        train = datasets.MNIST(data_path, train=True, download=download, transform=transform_train)\n",
        "        val = datasets.MNIST(data_path, train=False, download=download, transform=transform_train)\n",
        "    elif dataset == \"Fashion-MNIST\":\n",
        "        train = datasets.FashionMNIST(data_path, train=True, download=download, transform=transform_train)\n",
        "        val = datasets.FashionMNIST(data_path, train=False, download=download, transform=transform_train)\n",
        "    elif dataset == \"SVHN\":\n",
        "        train = datasets.SVHN(data_path, train=True, download=download, transform=transform_train)\n",
        "        val = datasets.SVHN(data_path, train=False, download=download, transform=transform_train)\n",
        "    elif dataset == \"notMNIST\":\n",
        "        root = data_path\n",
        "        if download:\n",
        "            # data url\n",
        "            data_url = \"https://github.com/facebookresearch/Adversarial-Continual-Learning/raw/main/data/notMNIST.zip\"\n",
        "            zip_file_path = \"{}/notMNIST.zip\".format(root)\n",
        "            # retrieve data\n",
        "            print(\"Downloading notMNIST from https://github.com/facebookresearch/Adversarial-Continual-Learning/raw/main/data/notMNIST.zip\")\n",
        "            path, headers = urlretrieve(data_url, zip_file_path)\n",
        "            # unzip\n",
        "            with zipfile.ZipFile(zip_file_path, 'r') as obj:\n",
        "                obj.extractall(root)\n",
        "\n",
        "        train = datasets.ImageFolder(\"{}/notMNIST/Train\".format(root), transform=transform_train)\n",
        "        val = datasets.ImageFolder(\"{}/notMNIST/Train\".format(root), transform=transform_val)\n",
        "\n",
        "    else :\n",
        "        raise ValueError(\"{} not found\".format(dataset))\n",
        "    return train, val\n",
        "\n",
        "def get_transforms(is_train):\n",
        "    # train dataset transform\n",
        "    if is_train:\n",
        "        return transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.RandomResizedCrop(size=(224,224)),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "            ])\n",
        "    # test dataset transform\n",
        "    else :\n",
        "        return transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "        ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgqjRUftmPCz"
      },
      "source": [
        "# Check datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oXU_S8OmPCz"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "import matplotlib.pyplot as plt\n",
        "def print_img(dataset, idx):\n",
        "    img = dataset.__getitem__(idx)[0]\n",
        "    plt.figure()\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(to_pil_image(img), cmap='gray')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oai400pvmPC0"
      },
      "source": [
        "# Model : Vision Transformer ViT-B/16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUMIbQoTmPC0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import einops\n",
        "\n",
        "# takes an image as an input, divide it into patches, let it through the embedding layer\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            stride=patch_size,\n",
        "            kernel_size=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)               # (n, 3, 224, 224) -> (n, embed_dim, patch_size, patch_size, )\n",
        "        x = torch.flatten(x, start_dim=2, end_dim=3)  # (n, embed_dim, patch_size*patch_size)\n",
        "        x = torch.transpose(x,1,2)           # (n, patch_size*patch_size, embed_dim)\n",
        "        return x\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_head : int,\n",
        "                 dim : int,\n",
        "                 qkv_bias : bool = True):\n",
        "        super().__init__()\n",
        "        self.num_head = num_head\n",
        "        self.dim = dim\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
        "        self.dropout_attn = nn.Dropout(0.)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.dropout_proj = nn.Dropout(0.)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input : (n, num_patch+1, embed_dim)\n",
        "\n",
        "        num_samples, num_tokens, dim = x.shape\n",
        "        if dim != self.dim :\n",
        "            raise ValueError\n",
        "\n",
        "        # multi-head attention\n",
        "        # create qkv\n",
        "        x = self.qkv(x)\n",
        "        # decouple q k v\n",
        "        x = einops.rearrange(x, 'b  n (h d qkv) -> (qkv) b h n d', h=self.num_head, qkv=3)\n",
        "        q,k,v = x[0], x[1], x[2]\n",
        "\n",
        "        # Scaled dot product attention\n",
        "        # q * k / d ** 1/2\n",
        "        e = torch.einsum('bnqd, bnkd -> bnqk', q, k) / ((self.dim)**(1/2))\n",
        "        x = nn.Softmax()(e)\n",
        "        # * v\n",
        "        x = torch.einsum('bnqk, bnvd -> bnqd',x,v)\n",
        "        x = einops.rearrange(x,'b n q d -> b q (n d)')\n",
        "        # linear\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_head : int,\n",
        "                 embed_dim : int,\n",
        "                 expansion : int = 4,\n",
        "                 drop: float = 0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = AttentionLayer(num_head,embed_dim)\n",
        "        ## mlp\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, expansion * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(expansion * embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, input): #\n",
        "        x = self.norm1(input)\n",
        "        x = self.attention(x)\n",
        "        x += input\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x += residual\n",
        "        return x\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self,\n",
        "                embed_dim : int,\n",
        "                num_class : int):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_class = num_class\n",
        "        self.norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.proj = nn.Linear(self.embed_dim, self.num_class)\n",
        "    def forward(self, input):\n",
        "        x = einops.reduce(input ,'b n e -> b e', reduction=\"mean\")\n",
        "        x = self.norm(x)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 num_head:int,\n",
        "                 depth:int,\n",
        "                 embed_dim:int):\n",
        "        super().__init__(*[TransformerEncoderBlock(num_head,embed_dim) for _ in range(depth)])\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                num_head : int,\n",
        "                num_class : int,\n",
        "                batch_size : int,\n",
        "                img_size : int = 224,\n",
        "                patch_size : int = 16,\n",
        "                in_channels : int = 3,\n",
        "                embed_dim : int = 768,\n",
        "                depth : int = 12,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.embedding = PatchEmbed(img_size, patch_size, in_channels=in_channels, embed_dim=embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(batch_size,1,embed_dim)).cuda()\n",
        "        self.num_patch = (img_size // patch_size) ** 2\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1,self.num_patch + 1,embed_dim)).cuda()\n",
        "        self.transformer_encoder = TransformerEncoder(num_head, depth, embed_dim)\n",
        "        self.head = Head(embed_dim, num_class)\n",
        "\n",
        "    def forward(self, x): # input (img): (batch, 3, H, W)\n",
        "\n",
        "        x = self.embedding(x) # (batch, num_patches, embed_dim)\n",
        "\n",
        "        # add cls (batch, num_patches, embed_dim) + (batch, 1, embed_dim) = (batch,num_patches+1, embed_dim)\n",
        "        x = torch.cat((self.cls_token, x), dim=1)\n",
        "\n",
        "        # pos (batch, num_patches+1, embed_dim) + (num_patches+1, embed_dim)\n",
        "        x += self.pos_embedding # (batch, num_patches+1, embed_dim)\n",
        "\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlVgP2AImPC1"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRjhhdCjmPC1"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.num_class = 10\n",
        "        self.batch_size = 8\n",
        "        self.img_size = 224\n",
        "        self.patch_size = 16\n",
        "        self.in_channels = 3\n",
        "        self.embed_dim = 768\n",
        "        self.depth = 12\n",
        "        self.num_head = 12\n",
        "\n",
        "        self.datasets = \"CIFAR100\"\n",
        "        self.data_path = \"C:/Windows/System32/L2P-pytorch/file\"\n",
        "        self.summary_path = \"C:/Windows/System32/L2P-pytorch/summary\"\n",
        "\n",
        "        self.lr = 0.001\n",
        "        self.lr_momentum = 0.9\n",
        "        self.weight_decay = 5e-4\n",
        "\n",
        "        self.start_epoch = 0\n",
        "        self.epochs = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X753_QK7mPC1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def accuracy(output, y):\n",
        "    pred = output.topk(k=1).indices\n",
        "    pred = torch.reshape(pred, (-1,))\n",
        "    guess = pred - y\n",
        "    acc = len((guess == 0).nonzero()) / len(y) \n",
        "    return acc\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, writer):\n",
        "    losses = 0.\n",
        "    accs = 0.\n",
        "    # train mode\n",
        "    model.train()\n",
        "    # one epoch\n",
        "    pbar = tqdm(enumerate(train_loader),desc=\"Epoch:{} Train\".format(epoch),total=len(train_loader))\n",
        "    for i, (input, target) in pbar:\n",
        "        target = target.cuda()\n",
        "        input = input.cuda()\n",
        "        \n",
        "        #forward\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        # compute grad\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        output = output.float()\n",
        "        losses += loss.float().item()\n",
        "        accs += accuracy(output.data, target)\n",
        "        pbar.set_postfix({\"trian_loss\":{losses}, \"train_acc\":{accs}})\n",
        "        \n",
        "    \n",
        "        \n",
        "    losses /= len(train_loader)\n",
        "    accs /= len(train_loader)\n",
        "    print(\"Epoch:{} trian_loss:{}\".format(epoch,losses))\n",
        "    print(\"Epoch:{} train_acc:{}\".format(epoch,accs))\n",
        "    writer.add_scalar(\"Loss/train\", losses, epoch)\n",
        "    writer.add_scalar(\"Accuracy/train\", accs, epoch)\n",
        "    \n",
        "    return accs\n",
        "def validate(val_loader, model, criterion, epoch, writer):\n",
        "    losses = 0.\n",
        "    accs = 0.\n",
        "\n",
        "    # evaluation mode\n",
        "    model.eval()\n",
        "    pbar = tqdm(enumerate(val_loader),desc=\"Epoch:{} Val:\".format(epoch),total=len(val_loader))\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in pbar:\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "            output = output.float()\n",
        "            loss = loss.float().item()\n",
        "            \n",
        "            # measure accuracy and record loss\n",
        "            accs += accuracy(output.data, target)\n",
        "            losses += loss\n",
        "            pbar.set_postfix({\"trian_loss\":{losses}, \"train_acc\":{accs}})\n",
        "\n",
        "            \n",
        "        losses /= len(val_loader)\n",
        "        accs /= len(val_loader)\n",
        "        print(\"Epoch:{} val_loss:{}\".format(epoch,losses))\n",
        "        print(\"Epoch:{} val_acc:{}\".format(epoch,accs))\n",
        "        writer.add_scalar(\"Loss/val\", losses, epoch)\n",
        "        writer.add_scalar(\"Accuracy/val\", accs, epoch)\n",
        "        \n",
        "    return accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyneZAYBmPC2",
        "outputId": "be9ab321-cfa0-4154-e94c-c1b3136c9a56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "model params :  86415592\n",
            "current lr 1.00000e-03\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "<ipython-input-5-bea446f2a664>:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x = nn.Softmax()(e)\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "configs = Config()\n",
        "model = VisionTransformer(configs.num_head, configs.num_class, configs.batch_size)\n",
        "\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"cuda not enabled\")\n",
        "    raise ValueError\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "train_transforms = get_transforms(is_train=True)\n",
        "val_transforms = get_transforms(is_train=False)\n",
        "\n",
        "train_datasets, val_datasets = get_dataset(\"CIFAR10\", train_transforms, val_transforms, data_path=configs.data_path, download=True)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_datasets,\n",
        "    batch_size = configs.batch_size,\n",
        "    shuffle = True,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    train_datasets,\n",
        "    batch_size = configs.batch_size,\n",
        "    shuffle = True,\n",
        ")\n",
        "\n",
        "writer = SummaryWriter(log_dir=configs.summary_path)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), configs.lr, weight_decay=configs.weight_decay)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)\n",
        "\n",
        "best_acc = 0\n",
        "print(\"model params : \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "for epoch in range(configs.start_epoch, configs.epochs):\n",
        "    print(\"current lr {:.5e}\".format(optimizer.param_groups[0]['lr']))\n",
        "    train(train_loader, model, criterion, optimizer, epoch, writer)\n",
        "    lr_scheduler.step()\n",
        "    avg_acc = validate(val_loader, model, criterion, epoch, writer)\n",
        "    \n",
        "    is_best = avg_acc > best_acc\n",
        "    if is_best:\n",
        "        torch.save(model.state_dict(), \"{}/best.pth\".format(configs.summary_path))\n",
        "        best_acc = avg_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t18qbuznDDKr"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/L2P/summary"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
