{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2P impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datasets\n",
    "    - Split CIFAR-100 : Split to 10 tasks (10 classes per task)\n",
    "    - 5-datasets : CIFAR-10, MNIST, Fashion-MNIST, SVHN, notMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "# dataset name, transform_train, transform_val, data_path\n",
    "def get_dataset(dataset, transform_train, transform_val, data_path, download):\n",
    "    if dataset == \"CIFAR10\":\n",
    "        train = datasets.CIFAR10(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.CIFAR10(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"CIFAR100\":\n",
    "        train = datasets.CIFAR100(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.CIFAR100(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"MNIST\":\n",
    "        train = datasets.MNIST(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.MNIST(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"Fashion-MNIST\":\n",
    "        train = datasets.FashionMNIST(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.FashionMNIST(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"SVHN\":\n",
    "        train = datasets.SVHN(data_path, train=True, download=download, transform=transform_train)\n",
    "        val = datasets.SVHN(data_path, train=False, download=download, transform=transform_train)\n",
    "    elif dataset == \"notMNIST\":\n",
    "        root = data_path\n",
    "        if download:\n",
    "            # data url\n",
    "            data_url = \"https://github.com/facebookresearch/Adversarial-Continual-Learning/raw/main/data/notMNIST.zip\"\n",
    "            zip_file_path = \"{}/notMNIST.zip\".format(root)\n",
    "            # retrieve data \n",
    "            print(\"Downloading notMNIST from https://github.com/facebookresearch/Adversarial-Continual-Learning/raw/main/data/notMNIST.zip\")\n",
    "            path, headers = urlretrieve(data_url, zip_file_path)\n",
    "            # unzip\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as obj:\n",
    "                obj.extractall(root)\n",
    "        \n",
    "        train = datasets.ImageFolder(\"{}/notMNIST/Train\".format(root), transform=transform_train)\n",
    "        val = datasets.ImageFolder(\"{}/notMNIST/Train\".format(root), transform=transform_val)\n",
    "        \n",
    "    else :\n",
    "        raise ValueError(\"{} not found\".format(dataset))\n",
    "    return train, val\n",
    "\n",
    "def get_transforms(is_train):\n",
    "    # train dataset transform\n",
    "    if is_train:\n",
    "        return transforms.Compose([\n",
    "                transforms.PILToTensor(),\n",
    "                transforms.RandomResizedCrop(size=(224,224)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "            ])\n",
    "    # test dataset transform\n",
    "    else :\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "def print_img(dataset, idx):\n",
    "    img = dataset.__getitem__(idx)[0]\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(to_pil_image(img), cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model : Vision Transformer ViT-B/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "\n",
    "# takes an image as an input, divide it into patches, let it through the embedding layer\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            stride=patch_size, \n",
    "            kernel_size=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.proj(input)               # (n, 3, 224, 224) -> (n, embed_dim, patch_size, patch_size, ) \n",
    "        x = torch.flatten(x, start_dim=2, end_dim=3)  # (n, embed_dim, patch_size*patch_size)\n",
    "        x = torch.transpose(x,1,2)           # (n, patch_size*patch_size, embed_dim)\n",
    "        return x\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_head : int,\n",
    "                 dim : int, \n",
    "                 qkv_bias : bool = True):\n",
    "        super().__init__()\n",
    "        self.num_head = num_head\n",
    "        self.dim = dim\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.dropout_attn = nn.Dropout(0.)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout_proj = nn.Dropout(0.)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input : (n, num_patch+1, embed_dim)\n",
    "        \n",
    "        num_samples, num_tokens, dim = input.shape\n",
    "        if dim != self.dim :\n",
    "            raise ValueError\n",
    "        \n",
    "        # multi-head attention\n",
    "        # create qkv\n",
    "        x = self.qkv(input)\n",
    "        # decouple q k v \n",
    "        x = einops.rearrange(x, 'b  n (h d qkv) -> (qkv) b h n d', h=self.num_head, qkv=3)\n",
    "        q,k,v = x[0], x[1], x[2]\n",
    "        \n",
    "        # Scaled dot product attention\n",
    "        # q * k / d ** 1/2\n",
    "        e = torch.einsum('bnqd, bnkd -> bnqk', q, k) / ((self.dim)**(1/2))\n",
    "        x = nn.Softmax()(e)\n",
    "        # * v\n",
    "        x = torch.einsum('bnqk, bnvd -> bnqd',x,v)\n",
    "        x = einops.rearrange(x,'b n q d -> b q (n d)')\n",
    "        # linear \n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_head : int,\n",
    "                 embed_dim : int,\n",
    "                 expansion : int = 4, \n",
    "                 drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = AttentionLayer(num_head,embed_dim)\n",
    "        ## mlp\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, expansion * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(expansion * embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input): #\n",
    "        x = self.norm1(input)\n",
    "        x = self.attention(x)\n",
    "        x += input\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x += residual\n",
    "        return x\n",
    "        \n",
    "class Head(nn.Module):\n",
    "    def __init__(self,\n",
    "                embed_dim : int, \n",
    "                num_class : int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_class = num_class\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = einops.reduce(input ,'b n e -> b e', reduction=\"mean\")\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "        x = nn.Linear(self.embed_dim, self.num_class)(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 num_head:int,\n",
    "                 depth:int,\n",
    "                 embed_dim:int):\n",
    "        super().__init__(*[TransformerEncoderBlock(num_head,embed_dim) for _ in range(depth)])\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                num_head : int,\n",
    "                num_class : int,\n",
    "                batch_size : int,\n",
    "                img_size : int = 224, \n",
    "                patch_size : int = 16,\n",
    "                in_channels : int = 3,\n",
    "                embed_dim : int = 768,\n",
    "                depth : int = 12,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.embedding = PatchEmbed(img_size, patch_size, in_channels=in_channels, embed_dim=embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(batch_size,1,embed_dim))\n",
    "        self.num_patch = (img_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,self.num_patch + 1,embed_dim))\n",
    "        self.transformer_encoder = TransformerEncoder(num_head, depth, embed_dim)\n",
    "        self.head = Head(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, input): # input (img): (batch, 3, H, W)\n",
    "\n",
    "        x = self.embedding(input) # (batch, num_patches, embed_dim)\n",
    "\n",
    "        # add cls (batch, num_patches, embed_dim) + (batch, 1, embed_dim) = (batch,num_patches+1, embed_dim)\n",
    "        print(\"after embed\", x.shape)\n",
    "        x = torch.cat((self.cls_token, x), dim=1)\n",
    "        # pos (batch, num_patches+1, embed_dim) + (num_patches+1, embed_dim)\n",
    "        x += self.pos_embedding # (batch, num_patches+1, embed_dim)\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_class = 1000\n",
    "        self.batch_size = 128\n",
    "        self.img_size = 224\n",
    "        self.patch_size = 16\n",
    "        self.in_channels = 3\n",
    "        self.embed_dim = 768\n",
    "        self.depth = 12\n",
    "        self.num_head = 12\n",
    "        \n",
    "        self.datasets = \"CIFAR100\"\n",
    "        self.data_path = \"C:/Users/99san/Workspace/L2P/data\"\n",
    "        self.summary_path = \"C:/Users/99san/Workspace/L2P/summary\"\n",
    "        \n",
    "        self.lr = 0.001\n",
    "        self.lr_momentum = 0.9\n",
    "        self.weight_decay = 5e-4\n",
    "        \n",
    "        self.start_epoch = 0\n",
    "        self.epochs = 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, y):\n",
    "    pred = output.topk(k=1).values\n",
    "    pred = torch.reshape(pred, (-1,))\n",
    "    guess = pred - y\n",
    "    acc = len((guess == 0).nonzero()) / len(y) \n",
    "    return acc\n",
    "    \n",
    "def train(train_loader, model, criterion, optimizer, epoch, writer):\n",
    "    losses = 0.\n",
    "    accs = 0.\n",
    "    # train mode\n",
    "    model.train()\n",
    "    # one epoch\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "        \n",
    "        #forward\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "        \n",
    "        # compute grad\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        output = output.float()\n",
    "        losses += loss.float()\n",
    "        accs += accuracy(output.data, target)[0]\n",
    "        \n",
    "    losses /= len(train_loader)\n",
    "    accs /= len(train_loader)\n",
    "    writer.add_scalar(\"Loss/train\", losses, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", accs, epoch)\n",
    "    \n",
    "    return accs\n",
    "    \n",
    "def validate(val_loader, model, criterion, epoch, writer):\n",
    "    losses = 0.\n",
    "    accs = 0.\n",
    "\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "            \n",
    "            # measure accuracy and record loss\n",
    "            accs += accuracy(output.data, target)[0]\n",
    "            losses += loss\n",
    "            \n",
    "        losses /= len(train_loader)\n",
    "        accs /= len(train_loader)\n",
    "        writer.add_scalar(\"Loss/val\", losses, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val\", accs, epoch)\n",
    "        \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda not enabled\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\99san\\Workspace\\L2P\\L2P.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99san/Workspace/L2P/L2P.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99san/Workspace/L2P/L2P.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcuda not enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/99san/Workspace/L2P/L2P.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99san/Workspace/L2P/L2P.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(configs\u001b[39m.\u001b[39mstart_epoch, configs\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/99san/Workspace/L2P/L2P.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcurrent lr \u001b[39m\u001b[39m{:.5e}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "\u001b[1;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "configs = Config()\n",
    "model = VisionTransformer(configs.num_head, configs.num_class, configs.batch_size)\n",
    "\n",
    "train_transforms = get_transforms(is_train=True)\n",
    "val_transforms = get_transforms(is_train=False)\n",
    "\n",
    "train_datasets, val_datasets = get_dataset(\"notMNIST\", train_transforms, val_transforms, data_path=configs.data_path, download=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size = configs.batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    train_datasets,\n",
    "    batch_size = configs.batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), configs.lr, weight_decay=configs.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=0)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"cuda not enabled\")\n",
    "    raise ValueError\n",
    "\n",
    "for epoch in range(configs.start_epoch, configs.epochs):\n",
    "    print(\"current lr {:.5e}\".format(optimizer.param_groups[0]['lr']))\n",
    "    train(train_loader, model, criterion, optimizer, epoch, writer)\n",
    "    lr_scheduler.step()\n",
    "    avg_acc = validate(val_loader, model, criterion, epoch)\n",
    "    \n",
    "    is_best = avg_acc > best_acc\n",
    "    if is_best:\n",
    "        torch.save(model.state_dict(), \"{}/best.pth\".format(configs.summary_path))\n",
    "        best_acc = avg_acc\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
